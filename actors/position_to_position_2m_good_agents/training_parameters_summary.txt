=== TRAINING PARAMETERS SUMMARY ===
Generated on: 2025-10-27 18:51:22 [Generated during training initialization]
Run Name: 2025_10_27_18_51_22_d+o+a+r+h+c+f+w+e+_888

=== TRAINING TYPE AND LOCATIONS ===
Training Date: 2025_10_27_18_51_22
Training Type: Full Multi-Component Training
Starting Location: (0.0, 0.0, 0.0)
Target Location: (0, 2, 0) [Position-to-Position training]

=== ABLATION STUDY FEATURES ===
Feature String from Run Name: d+o+a+r+h+c+f+w+e+
=== ACTUAL CONFIG VALUES ===
Disturbance (d): Enabled
Observation Noise (o): Enabled
Asymmetric Actor-Critic (a): Enabled
Rotor Delay (r): Enabled
Action History (h): Enabled
Curriculum Learning (c): Enabled
Recalculate Rewards (f): Enabled
Initial Reward Function (w): Enabled
Exploration Noise Decay (e): Enabled
Init Normal (init): Enabled
Position-to-Position Reward: Enabled

=== ACTOR INITIALIZATION ===
Starting Actor: Initialized from checkpoint
Initialization Type: Pre-trained weights loaded
Starting Actor File: actors/hoverActor_000000000300000.h
Checkpoint Path: actors/hoverActor_000000000300000.h
Base Seed: 0 (CONFIG::BASE_SEED)
Warmup Steps (Critic): 15000 steps
Warmup Steps (Actor): 30000 steps

=== TRAINING STEPS ===
Maximum Training Steps: 3000001
Actor Checkpoint Interval: 200000 steps
Actor Checkpoints Enabled: Yes
Total Expected Checkpoints: 15

=== REWARD FUNCTION PARAMETERS ===
Reward Function Type: Position-to-Position (Hover-like parameters)
Selected Function: reward_position_to_position_hover_like
=== REWARD FUNCTION NUMERICAL VALUES ===
Non-negative: false
Scale: 0.2
Constant: 4
Termination Penalty: -100
Position Weight: 1.5
Orientation Weight: 4
Linear Velocity Weight: 1.5
Angular Velocity Weight: 0.2
Linear Acceleration Weight: 0.2
Angular Acceleration Weight: 0.2
Action Baseline: 0.334
Action Penalty: 0.02
=== POSITION-TO-POSITION SPECIFIC PARAMETERS ===
Target Position: (0, 2, 0)
Target Radius: 0.1 (from reward_position_to_position_hover_like definition)
Velocity Reward Scale: 0.5 (from reward_position_to_position_hover_like definition)
Use Target Progress: false (from reward_position_to_position_hover_like definition)
=== REWARD FUNCTION EXPLANATION ===
Higher position/orientation weights = stronger penalty for deviation
Higher action penalty = smoother control but potentially slower convergence
Scale factor affects overall reward magnitude
Constant provides baseline reward level

=== ENVIRONMENT CONFIGURATION ===
Simulation Type: Multirotor (Crazyflie 2.0)
Physics Engine: Custom RL-Tools Multirotor Simulator
Integration Method: RK4 (Runge-Kutta 4th order)
Simulation Step Size (dt): 0.01 seconds
Episode Length: 2000 steps (20 seconds)
Evaluation Episode Length: 2000 steps (20 seconds)
Number of Parallel Environments: 1
Action Dimension: 4 (motor commands)
Observation Dimension: 146
Observation Dimension Privileged: 28

=== TERMINATION CONFIGURATION ===
Termination Enabled: Yes
Position Threshold: 3.5 meters (any axis)
Linear Velocity Threshold: 1000 m/s
Angular Velocity Threshold: 1000 rad/s
Note: Episode terminates if any position, velocity, or angular velocity exceeds threshold

=== ALGORITHM PARAMETERS ===
Algorithm: TD3 (Twin Delayed Deep Deterministic Policy Gradient)
Actor Batch Size: 256
Critic Batch Size: 256
Training Interval: 10 steps
Actor Training Interval: 20 steps
Critic Training Interval: 10 steps
Target Network Update Interval (Actor): 20 steps
Target Network Update Interval (Critic): 10 steps
Discount Factor (Gamma): 0.99
Target Action Noise STD: 0.2
Target Action Noise Clip: 0.5
Ignore Termination: No
Replay Buffer Capacity: 3000001
Off-Policy Runner Exploration Noise: 0.1
Evaluation Interval: 10000 steps
Number of Evaluation Episodes: 1000

=== CHECKPOINTING CONFIGURATION ===
Checkpoint Interval: Every 200000 steps
Checkpoints Enabled: Yes
File Formats Saved:
  - HDF5 (.h5): For analysis and evaluation
  - C++ Header (.h): For embedded deployment
TensorBoard Logging: Enabled
Learning Curve Data: Saved to HDF5 format
Deterministic Evaluation: No
Collect Episode Stats: No

=== ADDITIONAL NOTES ===
This summary was generated automatically during the training process.
Parameters are extracted from compile-time configuration and runtime settings.
For exact template parameters, refer to the CONFIG type used in training.

=== RELATED FILES ===
Actor Checkpoints: ../actors/ (actor_*.h5, actor_*.h)
TensorBoard Logs: ./data.tfevents
